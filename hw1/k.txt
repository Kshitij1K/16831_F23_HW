########################
logging outputs to  /home/kshitij/IRL_Assignments/16831_F23_HW/hw1/rob831/scripts/../../data/q2_dagger_hopper_Hopper-v2_21-09-2023_18-54-59
########################
Using GPU id 0
Using GPU id 0
Loading expert policy from... rob831/policies/experts/Hopper.pkl
obs (1, 11) (1, 11)
Done restoring expert policy...


********** Iteration 0 ************

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
317
709
1057
1279
1442
1675
1945
2257
2485
2748
2899
3061
3210
3444
3678
4040
4267
4555
4829
5101
Eval_AverageReturn : 805.3884887695312
Eval_StdReturn : 254.06129455566406
Eval_MaxReturn : 1259.5030517578125
Eval_MinReturn : 382.7875061035156
Eval_AverageEpLen : 255.05
Train_AverageReturn : 3772.67041015625
Train_StdReturn : 1.9483642578125
Train_MaxReturn : 3774.61865234375
Train_MinReturn : 3770.721923828125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 0
TimeSinceStart : 2.7844021320343018
Training Loss : 0.042727530002593994
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...




********** Iteration 1 ************

Collecting data to be used for training...
233
366
546
850
996
1098
1259
1541
1679
2112
2342
2599
2872
3238
3628
4001
4145
4311
4578
4680
5020

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
327
708
1009
1353
1705
2015
2316
2600
3110
3414
3716
4109
4549
4847
5150
Eval_AverageReturn : 1219.3148193359375
Eval_StdReturn : 238.6702423095703
Eval_MaxReturn : 1870.30322265625
Eval_MinReturn : 956.8423461914062
Eval_AverageEpLen : 343.3333333333333
Train_AverageReturn : 737.3244018554688
Train_StdReturn : 372.7135009765625
Train_MaxReturn : 1415.67529296875
Train_MinReturn : 215.6114044189453
Train_AverageEpLen : 239.04761904761904
Train_EnvstepsSoFar : 5020
TimeSinceStart : 6.490494251251221
Training Loss : 0.0644458681344986
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...




********** Iteration 2 ************

Collecting data to be used for training...
286
564
931
1233
1517
1792
2079
2433
2700
3074
3386
3754
4096
4439
4738
5012

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
1000
2000
3000
4000
5000
Eval_AverageReturn : 3715.65966796875
Eval_StdReturn : 14.075490951538086
Eval_MaxReturn : 3733.313720703125
Eval_MinReturn : 3694.15478515625
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 1084.855224609375
Train_StdReturn : 153.6315155029297
Train_MaxReturn : 1340.3980712890625
Train_MinReturn : 896.1377563476562
Train_AverageEpLen : 313.25
Train_EnvstepsSoFar : 10032
TimeSinceStart : 10.67008638381958
Training Loss : 0.03346988931298256
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...




********** Iteration 3 ************

Collecting data to be used for training...
1000
2000
3000
4000
5000

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
1000
2000
3000
4000
5000
Eval_AverageReturn : 3761.49951171875
Eval_StdReturn : 13.890254974365234
Eval_MaxReturn : 3782.0283203125
Eval_MinReturn : 3739.85888671875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 3700.676513671875
Train_StdReturn : 17.514328002929688
Train_MaxReturn : 3731.949462890625
Train_MinReturn : 3678.98046875
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 15032
TimeSinceStart : 14.707746744155884
Training Loss : 0.02943291701376438
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...




********** Iteration 4 ************

Collecting data to be used for training...
1000
2000
3000
3519
4519
5519

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
1000
2000
3000
4000
5000
Eval_AverageReturn : 3636.461669921875
Eval_StdReturn : 21.491504669189453
Eval_MaxReturn : 3672.09130859375
Eval_MinReturn : 3615.14208984375
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 3456.243408203125
Train_StdReturn : 659.3074951171875
Train_MaxReturn : 3760.30908203125
Train_MinReturn : 1982.10546875
Train_AverageEpLen : 919.8333333333334
Train_EnvstepsSoFar : 20551
TimeSinceStart : 18.96409010887146
Training Loss : 0.01747712306678295
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...




********** Iteration 5 ************

Collecting data to be used for training...
1000
2000
3000
4000
5000

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
756
1756
2756
3756
4756
5180
Eval_AverageReturn : 3259.700439453125
Eval_StdReturn : 826.552734375
Eval_MaxReturn : 3785.47900390625
Eval_MinReturn : 1568.86767578125
Eval_AverageEpLen : 863.3333333333334
Train_AverageReturn : 3656.034423828125
Train_StdReturn : 33.14698791503906
Train_MaxReturn : 3691.195068359375
Train_MinReturn : 3611.701904296875
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 25551
TimeSinceStart : 23.029122591018677
Training Loss : 0.020354261621832848
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...




********** Iteration 6 ************

Collecting data to be used for training...
1000
1506
2428
3428
4376
5159

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
1000
2000
3000
4000
5000
Eval_AverageReturn : 3747.729248046875
Eval_StdReturn : 7.543168544769287
Eval_MaxReturn : 3760.114013671875
Eval_MinReturn : 3738.79443359375
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 3275.205078125
Train_StdReturn : 665.4041748046875
Train_MaxReturn : 3787.7529296875
Train_MinReturn : 1922.8687744140625
Train_AverageEpLen : 859.8333333333334
Train_EnvstepsSoFar : 30710
TimeSinceStart : 27.294807195663452
Training Loss : 0.0143150445073843
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...




********** Iteration 7 ************

Collecting data to be used for training...
1000
2000
3000
4000
5000

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
1000
2000
3000
4000
5000
Eval_AverageReturn : 3767.30810546875
Eval_StdReturn : 5.218597888946533
Eval_MaxReturn : 3776.998779296875
Eval_MinReturn : 3761.474609375
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 3745.25244140625
Train_StdReturn : 4.676003456115723
Train_MaxReturn : 3751.125732421875
Train_MinReturn : 3738.197021484375
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 35710
TimeSinceStart : 31.94265055656433
Training Loss : 0.012268775142729282
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...




********** Iteration 8 ************

Collecting data to be used for training...
1000
2000
3000
4000
5000

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
1000
2000
3000
4000
5000
Eval_AverageReturn : 3772.176513671875
Eval_StdReturn : 3.2034425735473633
Eval_MaxReturn : 3775.921875
Eval_MinReturn : 3766.4580078125
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 3768.501220703125
Train_StdReturn : 5.651343822479248
Train_MaxReturn : 3776.884765625
Train_MinReturn : 3761.32080078125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 40710
TimeSinceStart : 36.180213928222656
Training Loss : 0.007023342419415712
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...




********** Iteration 9 ************

Collecting data to be used for training...
1000
2000
3000
4000
5000

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
1000
2000
3000
4000
5000
Eval_AverageReturn : 3770.44775390625
Eval_StdReturn : 2.686861991882324
Eval_MaxReturn : 3772.604248046875
Eval_MinReturn : 3765.247314453125
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 3775.505126953125
Train_StdReturn : 2.495695114135742
Train_MaxReturn : 3779.62255859375
Train_MinReturn : 3772.45849609375
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 45710
TimeSinceStart : 40.300938844680786
Training Loss : 0.00863590743392706
Initial_DataCollection_AverageReturn : 3772.67041015625
Done logging...


