'''
********** Iteration 0 ************

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
300
403
1403
2403
3403
3778
4778
5778
Eval_AverageReturn : 1502.934326171875
Eval_StdReturn : 713.6718139648438
Eval_MaxReturn : 2389.7099609375
Eval_MinReturn : 264.3769836425781
Eval_AverageEpLen : 722.25
Train_AverageReturn : 4713.6533203125
Train_StdReturn : 12.196533203125
Train_MaxReturn : 4725.849609375
Train_MinReturn : 4701.45654296875
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 0
TimeSinceStart : 3.8094568252563477
Training Loss : 0.03462616726756096
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 1 ************

Collecting data to be used for training...
744
926
1926
2926
3642
4642
5642

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
1000
1262
2262
2371
2535
3535
4535
5437
Eval_AverageReturn : 2786.5439453125
Eval_StdReturn : 1637.6800537109375
Eval_MaxReturn : 4221.53466796875
Eval_MinReturn : 410.28460693359375
Eval_AverageEpLen : 679.625
Train_AverageReturn : 1704.642333984375
Train_StdReturn : 668.171875
Train_MaxReturn : 2599.21435546875
Train_MinReturn : 546.7576904296875
Train_AverageEpLen : 806.0
Train_EnvstepsSoFar : 5642
TimeSinceStart : 9.270844459533691
Training Loss : 0.013083464466035366
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 2 ************

Collecting data to be used for training...
1000
1489
2489
3489
4489
5441

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
1000
2000
3000
4000
5000
Eval_AverageReturn : 4429.0146484375
Eval_StdReturn : 79.98484802246094
Eval_MaxReturn : 4542.06396484375
Eval_MinReturn : 4321.2763671875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 3819.3203125
Train_StdReturn : 834.3202514648438
Train_MaxReturn : 4474.8701171875
Train_MinReturn : 1989.7667236328125
Train_AverageEpLen : 906.8333333333334
Train_EnvstepsSoFar : 11083
TimeSinceStart : 14.4664306640625
Training Loss : 0.006641359068453312
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 3 ************

Collecting data to be used for training...
1000
2000
3000
4000
5000

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
1000
2000
2938
3938
4938
5938
Eval_AverageReturn : 4171.70068359375
Eval_StdReturn : 275.7729187011719
Eval_MaxReturn : 4492.23486328125
Eval_MinReturn : 3704.114990234375
Eval_AverageEpLen : 989.6666666666666
Train_AverageReturn : 4473.40380859375
Train_StdReturn : 112.30184936523438
Train_MaxReturn : 4624.609375
Train_MinReturn : 4328.3154296875
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 16083
TimeSinceStart : 19.85073161125183
Training Loss : 0.004336844198405743
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 4 ************

Collecting data to be used for training...
1000
2000
3000
4000
5000

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
1000
2000
3000
4000
5000
Eval_AverageReturn : 4510.40869140625
Eval_StdReturn : 75.34380340576172
Eval_MaxReturn : 4592.63427734375
Eval_MinReturn : 4404.30419921875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4162.18212890625
Train_StdReturn : 301.4471435546875
Train_MaxReturn : 4366.95263671875
Train_MinReturn : 3563.13330078125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 21083
TimeSinceStart : 24.92793035507202
Training Loss : 0.00314398854970932
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 5 ************

Collecting data to be used for training...
1000
2000
3000
4000
5000

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
1000
2000
3000
4000
5000
Eval_AverageReturn : 4763.125
Eval_StdReturn : 65.79873657226562
Eval_MaxReturn : 4883.7998046875
Eval_MinReturn : 4694.5654296875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4387.7626953125
Train_StdReturn : 63.37172317504883
Train_MaxReturn : 4502.36669921875
Train_MinReturn : 4322.5693359375
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 26083
TimeSinceStart : 30.036784648895264
Training Loss : 0.0019852316472679377
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 6 ************

Collecting data to be used for training...
1000
2000
3000
4000
5000

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
1000
2000
3000
4000
5000
Eval_AverageReturn : 4353.427734375
Eval_StdReturn : 426.3916015625
Eval_MaxReturn : 4726.056640625
Eval_MinReturn : 3520.7607421875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4653.87939453125
Train_StdReturn : 81.31298065185547
Train_MaxReturn : 4798.44677734375
Train_MinReturn : 4570.90966796875
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 31083
TimeSinceStart : 35.153764724731445
Training Loss : 0.0015010969946160913
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 7 ************

Collecting data to be used for training...
1000
2000
3000
4000
5000

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
1000
2000
3000
4000
5000
Eval_AverageReturn : 4709.4755859375
Eval_StdReturn : 87.79226684570312
Eval_MaxReturn : 4807.51171875
Eval_MinReturn : 4568.5478515625
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4607.68212890625
Train_StdReturn : 48.346675872802734
Train_MaxReturn : 4692.29296875
Train_MinReturn : 4542.08203125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 36083
TimeSinceStart : 40.345595359802246
Training Loss : 0.0013348590582609177
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 8 ************

Collecting data to be used for training...
1000
2000
3000
4000
5000

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
1000
2000
3000
4000
5000
Eval_AverageReturn : 4769.88037109375
Eval_StdReturn : 106.98566436767578
Eval_MaxReturn : 4979.716796875
Eval_MinReturn : 4679.7265625
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4747.794921875
Train_StdReturn : 80.02072143554688
Train_MaxReturn : 4874.99755859375
Train_MinReturn : 4642.521484375
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 41083
TimeSinceStart : 45.58318614959717
Training Loss : 0.0010856649605557323
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 9 ************

Collecting data to be used for training...
1000
2000
3000
4000
5000

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...
1000
1635
2635
2823
3823
4823
5823
Eval_AverageReturn : 3887.486083984375
Eval_StdReturn : 1381.046875
Eval_MaxReturn : 4932.705078125
Eval_MinReturn : 842.4674072265625
Eval_AverageEpLen : 831.8571428571429
Train_AverageReturn : 4695.7265625
Train_StdReturn : 45.79258728027344
Train_MaxReturn : 4771.6337890625
Train_MinReturn : 4641.11669921875
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 46083
TimeSinceStart : 51.17759823799133
Training Loss : 0.0008654121193103492
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...
'''

# plot the Eval_AverageReturn vs iteration number from the comment above
import matplotlib.pyplot as plt

if __name__ == '__main__':
    eval_returns = [
        1502.934326171875,
        2786.5439453125,
        4429.0146484375,
        4171.70068359375,
        4510.40869140625,
        4763.125,
        4353.427734375,
        4709.4755859375,
        4769.88037109375,
        3887.486083984375
    ]
    
    stdreturn = [
        713.6718139648438,
        1637.6800537109375,
        79.98484802246094,
        275.7729187011719,
        75.34380340576172,
        65.79873657226562,
        426.3916015625,
        87.79226684570312,
        106.98566436767578,
        1381.046875
    ]

    iterations = [i for i in range(10)]

    # Also connect all the points with a line
    plt.plot(iterations, eval_returns)
    
    plt.errorbar(iterations, eval_returns, stdreturn, linestyle='None', marker='^')

    # Draw a horizontal line at y=4713.6533203125
    plt.axhline(y=4713.6533203125, color='r', linestyle='-')

    # Draw a horizontal line at y=1502.93
    plt.axhline(y=1502.93, color='g', linestyle='-')

    # Increase the size of these labels
    plt.xlabel('Iteration number', fontsize=35)
    plt.ylabel('Average evaluation return', fontsize=25)
    plt.title('Average Evaluation Return vs. Iteration number, Ant', fontsize=25)
    plt.legend(['DAgger', 'Expert Policy', 'Behavior Cloning'], loc='lower right')
    plt.show()